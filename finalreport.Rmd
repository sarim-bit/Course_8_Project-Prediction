---
title: "Practical Machine Learning Project"
author: "Sarim"
date: "20/06/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

We will be using decision trees and random forest methods for predictions

# Data Processing
## Libraries Used
```{r cache=TRUE}

library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
```

## Loading Data
```{r cache=TRUE}
#Before doing any operations we set the seed to maintain reproducibility
set.seed(12345)
training<-read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!", ""))
testing<-read.csv("pml-testing.csv",na.strings=c("NA","#DIV/0!", ""))
```
## Cleaning Data
The datasets loaded have a few columns which have no results and the first 7 columns have no information which will help us in the predictions. So we remove these columns for both the datasets. Also the predicted variable *classe* is a factor but present as a character and so it is converted into a factor variable.

```{r}
training<-training[,colSums(is.na(training))==0]
testing<-testing[,colSums(is.na(testing))==0]
training<-training[,-c(1:7)]
testing<-testing[,-c(1:7)]

# Converting classe to factor type
training$classe<-as.factor(training$classe)
```

## Partitioning of data 
The testing set is kept aside for final test just once and the training set is divided into two subsets - for training and cross validation.
The two datasets are created using *createDataPartition* with a prob distribution of 60% and 40% in favour of training.

```{r}
inTrain<-createDataPartition(training$classe,p=0.6,list=FALSE)
newTraining<-training[inTrain,]
newTesting<-training[-inTrain,]
```

# Prediction
Before diving into the prediction we first look the variable we are dealing with and do some simple analysis

```{r}
summary(newTraining$classe)
plot(newTraining$classe)
```
From the plot we can see that *classe* is factor with 5 levels *A,B,C,D and E*. *A* level is significantly more while all others are close to each other.

## Decision Trees using rpart
```{r cache=TRUE}
fit1<-rpart(classe ~ ., data=newTraining, method="class")
prp(fit1)
pred1<-predict(fit1,newTesting,type = "class")
confusionMatrix(pred1,newTesting$classe)
```
As can be seen by the result of the confusion matrix this is not a very good predictor and gives an accuracy of about 75%.This model does not have good sensitivity but has decent specificity. So we look for a better method if possible.

## Random Forest
```{r cache=TRUE}
fit2<-randomForest(classe ~.,newTraining)
pred2<-predict(fit2,newTesting,type = "class")
confusionMatrix(pred2,newTesting$classe)
```
The random forest method provides great result with accuracy over 99%. This method is great for our data and no further improvement looks likely and so this method can be used for any further predictions related to this type of data.

## Final Test predictions
``` {r}
pred_test<-predict(fit2,testing,type = "class")
```
